{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодняшний герой дня - градиентный спуск - краеугольный камень машинного обучения. Градиент - это направление, в котором функция возрастает (антиградиент - в котором убывает). Это направление определяется производной функции (или частными производными для функций с несколькими переменными). Допустим нам надо подобрать значение некоторых параметров, при которых целевая функция будет минимальна. Для этого мы:\n",
    "\n",
    "1. Случайным образом инициируем искомые параметры.\n",
    "2. Рассчитаем значение целевой функции для этих параметров.\n",
    "3. Посчитаем значение градиента для этиx параметров.\n",
    "4. Изменим значения искомых параметров на значение градиента с определенным шагом. Т.е. вычтем или прибавим к искомым параметрам (полученным в пункте 1) значение градиента умноженного на какое-нибудь число a (например, 0,01, 0,1, 1 и т.д.). Так же это называется \"сделать градиентный шаг\". Сам параметр a подбирается из диапазона значений: на каждом шаге выбирается такой параметр, при котором целевая функция минимальна.\n",
    "5. Пересчитаем значение целевой функции, используя параметры полученные после градиентного шага.\n",
    "6. Проверяем, не сошелся ли градиентный спуск. Для этого можно использовать несколько способов. Можно сравнить новое целевое значение (пункт 5) со значение из пункта 2. Если оно изменилось менее чем определенную заданную точность (tolerance), то мы считаем, что схождение произошло. А можно так же измерять разницу евклидового расстояния между векторами параметров и тоже сравнивать ее с tolerance.\n",
    "7. Если имеем схождение, то искомые параметры найдены. Если нет - то запоминаем значения целевой функции и новых параметров, а затем повторяем действия с 3 по 6 пункт, пока градиент не сойдется.\n",
    "\n",
    "Как видно - это итеративный алгоритм, на практике количество итераций часто ограничивают. Давайте перейдем к практике.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем решать задачу линейной регрессии. Линейная регрессия это восстановление зависимости одной переменной (зависимой) от другой (не зависимой). Формула следующая:\n",
    "\n",
    "$$ y = \\beta*x + \\alpha + \\epsilon$$\n",
    "\n",
    "где y - зависимая переменная, x - независимая, e - ошибка (надеемся, что очень маленькая), которая зависит от факторов, неучтенных в данной простой модели.\n",
    "\n",
    "Альфа и бетта же - как раз те параметры, которые мы будем пытаться найти. Линейную регрессию можно восстановить с помощью метода наименьших квадратов (МНК). МНК - это метод, при котором минимизируется сумма квадратов ошибок между реальными значениями y и предсказанными значениями.\n",
    "\n",
    "$$ ошибка = y_{актуальный} - y_{предсказанный} $$\n",
    "\n",
    "В квадрат ошибка возводится для того, чтобы при суммировании положительные и отрицательные ошибки не выгашивали друг друга. Давайте рассчитаем производную для этой функции. Т.к. функция зависит от альфа и бетта, то градиент будет состоять из двух частных производных. А что если представить, что альфа тоже домножается на x, но этот x всегда равен единице? Уравнение регрессии от этого не измениться, а вот производную можно будет брать только по бета. Найдем же эту производную. Для начала разложим функцию ошибок в квадрате:\n",
    "\n",
    "ошибка^2 = (y - b*x - a)^2 = ((y-a)-b*x)^2 = (y-a)^2-2(y-a)*bx+(bx)^2\n",
    "\n",
    "Теперь можно продифференцировать по бетта (y,x и альфа - константы):\n",
    "\n",
    "(ошибка^2)' = -2x*(y-a) + 2bx = -2x*(y-a-b)\n",
    "\n",
    "Множитель в скобках это ни что иное, как функция ошибки, значит производную можно записать так:\n",
    "\n",
    "-2x*ошибка(x,y,бетта)\n",
    "\n",
    "Теперь облачим все это в форму кода. Чтобы сделать это с нуля, нам потребуется набор функций для работы с векторами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def dot(v, w):\n",
    "    \"\"\"перемножает и суммирует два вектора\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "def vector_add(v, w):\n",
    "    \"\"\"складывает два вектора по-элементно\"\"\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"вычитает два вектора по-элементно\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "def scalar_multiply(c, v):\n",
    "    \"\"\"умножает каждый элемент вектора на число\"\"\"\n",
    "    return [c * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала напишем функцию предсказания значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(x_i, beta):\n",
    "    return dot(x_i, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но где же альфа? Как я написал выше, мы будем считать, что альфа тоже домножается на признак, который всегда равен единице. Для этого к исходным данным нам нужно будет добавлять столбик из единиц. Данная функция принимает на входе вектора x и бетта, перемножает и суммирует их."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ошибки будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(x_i, y_i, beta):\n",
    "    return y_i - predict(x_i, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ошибки в квадрате:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error_i(x_i, y_i, beta):\n",
    "    return error(x_i, y_i, beta) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот и функция для градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error_gradient_i(x_i, y_i, beta):\n",
    "    return [-2 * x_ij * error(x_i, y_i, beta)\n",
    "            for x_ij in x_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эти функции написаны для i-ого элемента данных. Надо записать их так же для всех данных сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error(x,y,beta):\n",
    "    return sum([squared_error_i(x_i,y_i,beta) \n",
    "                for x_i, y_i in zip(x,y)])\n",
    "\n",
    "def squared_error_gradient(x,y,beta):\n",
    "    return reduce(vector_add,\n",
    "                  np.array([squared_error_gradient_i(x_i,y_i,beta)\n",
    "               for x_i,y_i in zip(x,y)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь сгенерируем данные, на которых будем проводить испытания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_regression\n",
    "x, y = make_regression(n_samples=100, n_features=1, n_informative=1, \n",
    "                        random_state=0, noise=35)\n",
    "# сразу же вставим колонку с единицами\n",
    "m, n = np.shape(x)\n",
    "x = np.c_[np.ones(m), x]\n",
    "x, y = x.tolist(), y.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня я буду реализовать версию пакетного (batch) градиетного спуска, т.е. того, который считает шаг на всех данных сразу. Поэтому значения x и y можно сразуже \"зашить\" в функцию ошибки и градиетна. Для этого используем функцию partial из functools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "target_fn = partial(squared_error,x,y)\n",
    "grad_fn = partial(squared_error_gradient,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно писать сам градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(target_fn,grad_fn,beta_0,num_iter,tolerance=0.000001):\n",
    "    \n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    beta = beta_0\n",
    "    value = target_fn(beta)\n",
    "    for i in range(num_iter):\n",
    "        grad = grad_fn(beta)\n",
    "        next_betas = [vector_subtract(beta,\n",
    "                                      scalar_multiply(step,grad)) \n",
    "                      for step in step_sizes] # делаем шаг\n",
    "        next_beta = min(next_betas, key=target_fn)\n",
    "        next_value = target_fn(next_beta)\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return beta\n",
    "        else:\n",
    "            beta, value = next_beta, next_value \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.8495390955931437, 43.204126172220924]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(target_fn,grad_fn,[0,0],10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Можно сделать этот код гораздо красивее без всяких вспомогательных функций. Для этого воспользуемся библиотекой numpy. Переведем x и y обратно в массивы numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потребуются так же функции ошибки, ошибки в квадрате и градиента квадратной ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_num(x,y,beta):\n",
    "    prediction = np.dot(x, beta)\n",
    "    return y - prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_error_num(x,y,beta):\n",
    "    return np.sum(error_num(x,y,beta)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_error_grad_num(x,y,beta):\n",
    "    return -2*np.dot(x.transpose(), error_num(x,y,beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем из них парциалы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_fn_num = partial(square_error_num,x,y)\n",
    "grad_fn_num = partial(square_error_grad_num,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама функция практически не изменится, за исключением строчки с расчетами шагов - теперь не надо использовать функции substract и scalar_multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_num(target_fn, grad_fn, steps, num_iter=10000,tolerance=0.000001):\n",
    "    beta = np.ones(2) # инициируем бетту\n",
    "    value = target_fn(beta)\n",
    "    for i in range(num_iter):\n",
    "        grad = grad_fn(beta)\n",
    "        # упростилась строчка с градиентным шагом\n",
    "        next_betas = [(beta-grad*step) for step in steps]\n",
    "        next_beta = min(next_betas, key=target_fn)\n",
    "        next_value = target_fn(next_beta)\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return beta\n",
    "        else:\n",
    "            beta, value = next_beta, next_value\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.84953788,  43.20412672])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "gradient_descent_num(target_fn_num,grad_fn_num,steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь конечно же проверим, что даст нам лёрновская линейная регрессия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x[:,1:],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 43.20424388]), -2.8496363946075389)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_,clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения очень похожи, а разняться наверное потому, что sklearn использует какой-то другой алгоритм для решения задачи регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск - очень понятный метод оптимизации целевой функции, который постоянно используется в решении задач машинного обучения. Его применяют при решении задач линейной регрессии, логистической регрессии, нейронных сетях и др. В данной статье была рассмотрена пакетная (batch) версия алгоритма градиентного спуска, который для расчета шага использует все данные **сразу**. Более популярный алгоритм - это стохастический градиентный спуск. Он считает шаг для каждого элемента выборки, которые берутся в случайном (стохастическом) порядке. Этот алгоритм позволяет решать задачи оптимизации на очень больших массивах данным, которые не могут храниться в оперативной памяти все сразу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная статья написанапо материалам совершенно замечальной книги по машинному обучению. Она называется [Data Science From Scratch](http://shop.oreilly.com/product/0636920033400.do) и написал ее Joel Grus. Очень рекомендую эту книгу, а так же [github автора](https://github.com/joelgrus/data-science-from-scratch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
